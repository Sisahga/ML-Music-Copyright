{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h1><b>COMP 432 - Machine Learning</b> | <b>Final Project</b> (W2025)</h1>\n",
    "<h3>Machine Learning Model to detect Copyright Infringement in Music</h3>\n",
    "Written by Sisahga Phimmasone - 40210015<br>\n",
    "April 20th, 2025\n"
   ],
   "id": "b41bf3e220dbace6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3><b>1. Data Preparation and Cleaning</b></h3>\n",
    "\n",
    "The code cells below convert YouTube video links of pairs of songs from the list of historically known copyright infringement cases to MP3.\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_songs_subject_to_plagiarism_disputes\n",
    "\n",
    "See 'copyright_cases.csv' file.\n",
    "Uses yt_dlp library to output YouTube video URLs to .mp3"
   ],
   "id": "850eb7e266f03ef8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h4><b>1.1 Convert YouTube URLs to MP3 with yt_dlp</b></h4>",
   "id": "9aa66d5ad417b613"
  },
  {
   "metadata": {
    "tags": [
     "Convert YouTube to MP3"
    ]
   },
   "cell_type": "code",
   "source": [
    "import yt_dlp\n",
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ],
   "id": "3917e72eaf277278",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Convert YouTube to MP3"
    ]
   },
   "cell_type": "code",
   "source": [
    "mp3_song_dir = \"songs_mp3\"\n",
    "os.makedirs(mp3_song_dir, exist_ok=True)\n",
    "df = pd.read_csv(\"copyright_cases.csv\")"
   ],
   "id": "c1ca5f777de7ce66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Convert YouTube to MP3"
    ]
   },
   "cell_type": "code",
   "source": [
    "# YT-DLP options\n",
    "def get_opts(output_path):\n",
    "    return {\n",
    "        'format': 'bestaudio/best',\n",
    "        'outtmpl': output_path,\n",
    "        'postprocessors': [{\n",
    "            'key': 'FFmpegExtractAudio',\n",
    "            'preferredcodec': 'mp3',\n",
    "            'preferredquality': '192',\n",
    "        }],\n",
    "        'quiet': False,\n",
    "        'noplaylist': True,\n",
    "    }"
   ],
   "id": "2e1341dd6b82d666",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Convert YouTube to MP3"
    ]
   },
   "cell_type": "code",
   "source": [
    "# Creates a clean filename for the mp3 files\n",
    "def clean_filename(s):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", s.replace(\" \", \"_\"))"
   ],
   "id": "27b350a3cf2e8bd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Convert YouTube to MP3"
    ]
   },
   "cell_type": "code",
   "source": [
    "# Download Loop, record by record\n",
    "for index, row in df.iterrows():\n",
    "    orig_song = clean_filename(row['Original Song'])\n",
    "    orig_artist = clean_filename(row['Original Artist'])\n",
    "    second_song = clean_filename(row['Second Song'])\n",
    "    second_artist = clean_filename(row['Second Artist'])\n",
    "    orig_url = row['Original Song YouTube Link']\n",
    "    second_url = row['Second Song Youtube Link']\n",
    "\n",
    "    row_id = f\"{index+1:03d}\"\n",
    "\n",
    "    # Build filenames\n",
    "    orig_filename = os.path.join(\n",
    "        mp3_song_dir,\n",
    "        f\"{row_id}_original_{orig_song}_by_{orig_artist}.%(ext)s\"\n",
    "    )\n",
    "    second_filename = os.path.join(\n",
    "        mp3_song_dir,\n",
    "        f\"{row_id}_second_{second_song}_by_{second_artist}.%(ext)s\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(get_opts(orig_filename)) as ydl:\n",
    "            print(f\"Downloading original: {orig_song} by {orig_artist}\")\n",
    "            ydl.download([orig_url])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download original song at {orig_url}: {e}\")\n",
    "\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(get_opts(second_filename)) as ydl:\n",
    "            print(f\"Downloading second: {second_song} by {second_artist}\")\n",
    "            ydl.download([second_url])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download second song at {second_url}: {e}\")"
   ],
   "id": "76e323f8fd580e3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Convert YouTube to MP3"
    ]
   },
   "cell_type": "code",
   "source": [
    "# Check how many files were downloaded (should be 176)\n",
    "mp3_files = [f for f in os.listdir(mp3_song_dir) if f.lower().endswith(\".mp3\")]\n",
    "print(f\"Total MP3 files downloaded: {len(mp3_files)}\")"
   ],
   "id": "af5fbd2f7f7a798c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h4><b>1.2 Create Data Structure for known copyrighted song pairings</b></h4>",
   "id": "b560301a5ad5b7a2"
  },
  {
   "metadata": {
    "tags": [
     "Preprocess known copyright pairs"
    ]
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import pickle"
   ],
   "id": "f96d080db2cb9f30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Preprocess known copyright pairs"
    ]
   },
   "cell_type": "code",
   "source": [
    "# Librosa Configuration\n",
    "AUDIO_DIR = mp3_song_dir\n",
    "SR = 16000\n",
    "DURATION = 10\n",
    "TARGET_LEN = DURATION * SR"
   ],
   "id": "c78584cd90cef836",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Preprocess known copyright pairs"
    ]
   },
   "cell_type": "code",
   "source": [
    "def preprocess_audio(audio_path):\n",
    "    \"\"\"Preprocesses .mp3 audio files with specified Librosa Configuration above. Splits each song into chunks of 10 seconds.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        audio_path: str\n",
    "            Path to audio file\n",
    "    RETURNS\n",
    "    -------\n",
    "        list:\n",
    "            A list of numpy arrays, where each array is a 10-second audio chunk of size TARGET_LEN of a song.\n",
    "    \"\"\"\n",
    "\n",
    "    audio, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "    # Trim silent parts (set to top off lower than 20DB)\n",
    "    audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "\n",
    "    # Resample to 16kHz for Wave2Vec2\n",
    "    if sr != SR:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=SR)\n",
    "\n",
    "    # Split song into 10-second chunks of 16kHz\n",
    "    segments = []\n",
    "    for i in range(0, len(audio), TARGET_LEN):\n",
    "        segment = audio[i: (i + TARGET_LEN)]\n",
    "        # If the segment is shorter than TARGET_LEN (i.e. last part of song), pad with zeros for uniformity\n",
    "        if len(segment) < TARGET_LEN:\n",
    "            segment = np.pad(segment, (0, TARGET_LEN - len(segment)), mode='constant')\n",
    "\n",
    "        segments.append(segment)\n",
    "\n",
    "    return segments"
   ],
   "id": "b217ff92343e4b5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Preprocess known copyright pairs"
    ]
   },
   "cell_type": "code",
   "source": [
    "def get_pair_id(audio_filename):\n",
    "    \"\"\"Retrieves pairs of known copyrighted cases from the songs_mp3 directory.\n",
    "\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    audio_filename: str\n",
    "        Path to audio file\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "        str:\n",
    "            The pair ID (i.e. \"001\")\n",
    "    \"\"\"\n",
    "    match = re.match(r\"(\\d{3})_\", audio_filename)\n",
    "    return match.group(1) if match else None"
   ],
   "id": "b493a3ce18c39d54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<b>Preprocess Known Copyrighted Pairs</b>",
   "id": "5586acf4e34b423c"
  },
  {
   "metadata": {
    "tags": [
     "Preprocess known copyright pairs"
    ]
   },
   "cell_type": "code",
   "source": [
    "# Group song mp3 files by pair ID\n",
    "pairs = {}\n",
    "for fname in os.listdir(mp3_song_dir):\n",
    "    if fname.endswith(\".mp3\"):\n",
    "        pair_id = get_pair_id(fname)\n",
    "        if pair_id:\n",
    "            pairs.setdefault(pair_id, []).append(fname)"
   ],
   "id": "601a3aa41f006b06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Preprocess known copyright pairs"
    ]
   },
   "cell_type": "code",
   "source": [
    "# Build dataset from the pairs dictionary intialized in the cell above, preprocess them, and save to pickle file\n",
    "similar_dataset = []\n",
    "preprocessed_output_dir = \"preprocessed_pairs\"\n",
    "os.makedirs(preprocessed_output_dir, exist_ok=True)\n",
    "\n",
    "for pair_id, files in pairs.items():\n",
    "    if len(files) < 2:\n",
    "        print(f\"Skipping {pair_id} expected 2 files but got {len(files)}\")\n",
    "        continue\n",
    "\n",
    "    files = sorted(files) # Original songs [0], copies [1]\n",
    "    og_song = os.path.join(mp3_song_dir, files[0])\n",
    "    copy_song = os.path.join(mp3_song_dir, files[1])\n",
    "\n",
    "    print(f\"Processing pair {pair_id}...\")\n",
    "    segments1 = preprocess_audio(og_song)\n",
    "    print(f\"Preprocessed original song segments: {len(segments1)}\")\n",
    "    segments2 = preprocess_audio(copy_song)\n",
    "    print(f\"Preprocessed copied song segments: {len(segments2)}\")\n",
    "\n",
    "    label = True # True since we are preprocessing all pairs of known copyright infringement cases\n",
    "\n",
    "    pair_data = {\n",
    "        \"pair_id\": pair_id,\n",
    "        \"original_song_segments\": segments1,\n",
    "        \"copied_song_segments\": segments2,\n",
    "        \"label\": label,\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(preprocessed_output_dir, f\"{pair_id}.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(pair_data, f)\n",
    "    print(f\"Saved pair {pair_id} to pickle.\")\n",
    "\n",
    "    similar_dataset.append(pair_data)"
   ],
   "id": "bb96d8da14c79962",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<b>Preprocess Dissimilar Pairs</b>",
   "id": "2f3e1816845feac2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T21:41:55.162735Z",
     "start_time": "2025-04-16T21:41:55.155411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an equal amount of dissimilar pairs from the same song bank.\n",
    "import random\n",
    "DISSIMILAR_PAIR_COUNT = 88"
   ],
   "id": "6caace4a5770a77b",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get the next pair ID number\n",
    "existing_ids = sorted(int(k) for k in pairs.keys())\n",
    "next_pair_id = max(existing_ids) + 1 if existing_ids else 0\n",
    "\n",
    "# Flatten the list of all .mp3 song files and shuffle\n",
    "all_files = [fname for fnames in pairs.values() for fname in fnames]\n",
    "random.shuffle(all_files)\n",
    "\n",
    "used_dissimilar_keys = set()\n",
    "dissimilar_dataset = []\n",
    "\n",
    "# Create dissimilar pairs\n",
    "while len(dissimilar_dataset) < DISSIMILAR_PAIR_COUNT:\n",
    "    file1, file2 = random.sample(all_files, 2) # Randomly select files from the list of songs.\n",
    "\n",
    "    id1 = get_pair_id(file1)\n",
    "    id2 = get_pair_id(file2)\n",
    "\n",
    "    # Ensure we don't have duplicate pairings and that we don't end up with an original pairing\n",
    "    if id1 != id2:\n",
    "        key = tuple(sorted((file1, file2)))\n",
    "        if key in used_dissimilar_keys:\n",
    "            continue\n",
    "\n",
    "        path1 = os.path.join(mp3_song_dir, file1)\n",
    "        path2 = os.path.join(mp3_song_dir, file2)\n",
    "\n",
    "        print(f\"Processing dissimilar pair: {file1} vs {file2}\")\n",
    "        segments1 = preprocess_audio(path1)\n",
    "        segments2 = preprocess_audio(path2)\n",
    "\n",
    "        # Format new pair ID as 3-digit string\n",
    "        new_pair_id = f\"{next_pair_id:03d}\"\n",
    "        next_pair_id += 1\n",
    "\n",
    "        pair_data = {\n",
    "            \"pair_id\": new_pair_id,\n",
    "            \"original_song_segments\": segments1,\n",
    "            \"copied_song_segments\": segments2,\n",
    "            \"label\": False,  # Unsimilar songs, so label is False\n",
    "        }\n",
    "\n",
    "        print(f\"Saved pair {new_pair_id} to pickle.\")\n",
    "        with open(os.path.join(preprocessed_output_dir, f\"{new_pair_id}.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(pair_data, f)\n",
    "        print(f\"Saved dissimilar pair {new_pair_id} to pickle.\")\n",
    "\n",
    "        dissimilar_dataset.append(pair_data)\n",
    "        used_dissimilar_keys.add(key)"
   ],
   "id": "8fb78848784d1904",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<b>Helper function to load preprocessed dataset from above</b>",
   "id": "3f8a121b89bb4e11"
  },
  {
   "metadata": {
    "tags": [
     "Load preprocessed song pairs"
    ],
    "ExecuteTime": {
     "end_time": "2025-04-16T21:49:17.699881Z",
     "start_time": "2025-04-16T21:49:17.696396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_dataset_from_pickles(pickle_dir):\n",
    "    \"\"\"\n",
    "    Handy function to load all preprocessed song pair .pkl files from preprocessed_pairs directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pickle_dir : str\n",
    "        Directory where the pickle files are stored.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of dictionaries, each containing:\n",
    "            - 'pair_id': str\n",
    "            - 'song1': list of numpy arrays (segments)\n",
    "            - 'song2': list of numpy arrays (segments)\n",
    "            - 'label': bool\n",
    "    \"\"\"\n",
    "    loaded_dataset = []\n",
    "    for pkl_fname in os.listdir(pickle_dir):\n",
    "        if pkl_fname.endswith(\".pkl\"):\n",
    "            path = os.path.join(pickle_dir, pkl_fname)\n",
    "            with open(path, \"rb\") as f:\n",
    "                load_pair_data = pickle.load(f)\n",
    "                loaded_dataset.append(load_pair_data)\n",
    "\n",
    "    return loaded_dataset"
   ],
   "id": "1f067f7f38c093d4",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<br><h3><b>2. Feature Extraction and Vectorization of Preprocessed Data</b></h3>",
   "id": "8ef80a952fa498dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h4><b>2.1 Transform preprocessed raw waveforms into feature vector representations with Librosa</b></h4>",
   "id": "a65432495e3257eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T21:48:46.670772Z",
     "start_time": "2025-04-16T21:48:46.667514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa.feature as librosa_feature\n",
    "from tqdm import tqdm"
   ],
   "id": "2a97d0a93c2ccbee",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T21:48:47.643235Z",
     "start_time": "2025-04-16T21:48:47.630757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_features(audio_segment, sr=16000):\n",
    "    \"\"\"\n",
    "    Extracts a set of audio features for music similarity detection using librosa such as:\n",
    "        - Spectral Features (Timbre, the unique quality of a song's sound)\n",
    "        - Rhythmic Features\n",
    "        - Harmonic and Melodic Features\n",
    "        - Structural Features\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio_segment : numpy.ndarray\n",
    "        Raw audio waveform (preprocessed above)\n",
    "    sr : int\n",
    "        Sample rate of the audio waveform\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary that holds the aforementioned audio features\n",
    "    \"\"\"\n",
    "\n",
    "    features = {}\n",
    "    # SPECTRAL FEATURES\n",
    "\n",
    "    # MFCC - captures timbre characteristics (timbre is the unique quality of a song's sound)\n",
    "    mfccs = librosa_feature.mfcc(y=audio_segment, sr=sr, n_mfcc=20)\n",
    "    features['mfcc_mean'] = np.mean(mfccs, axis=1)\n",
    "    features['mfcc_std'] = np.std(mfccs, axis=1)\n",
    "\n",
    "    # Spectral contrast: captures timbral contrast between peaks and valleys\n",
    "    contrast = librosa_feature.spectral_contrast(y=audio_segment, sr=sr)\n",
    "    features['spectral_contrast_mean'] = np.mean(contrast, axis=1)\n",
    "\n",
    "    # Spectral centroid: brightness of sound\n",
    "    centroid = librosa_feature.spectral_centroid(y=audio_segment, sr=sr)[0]\n",
    "    features['spectral_centroid_mean'] = np.mean(centroid)\n",
    "    features['spectral_centroid_std'] = np.std(centroid)\n",
    "\n",
    "    # Spectral rolloff: frequency below which most energy is contained\n",
    "    rolloff = librosa_feature.spectral_rolloff(y=audio_segment, sr=sr)[0]\n",
    "    features['spectral_rolloff_mean'] = np.mean(rolloff)\n",
    "\n",
    "    # Width of spectrum\n",
    "    bandwidth = librosa_feature.spectral_bandwidth(y=audio_segment, sr=sr)[0]\n",
    "    features['spectral_bandwidth_mean'] = np.mean(bandwidth)\n",
    "\n",
    "    # 2. RHYTHM FEATURES\n",
    "\n",
    "    # Tempo and beat sterngth\n",
    "    onset_env = librosa.onset.onset_strength(y=audio_segment, sr=sr)\n",
    "    tempo, beat_frames = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "    features['tempo'] = tempo\n",
    "\n",
    "    # Beat intervals if beats were detected\n",
    "    if len(beat_frames) > 1:\n",
    "        beat_times = librosa.frames_to_time(beat_frames, sr=sr)\n",
    "        beat_intervals = np.diff(beat_times)\n",
    "        features['beat_intervals_mean'] = np.mean(beat_intervals)\n",
    "        features['beat_intervals_std'] = np.std(beat_intervals)\n",
    "    else:\n",
    "        features['beat_intervals_mean'] = 0\n",
    "        features['beat_intervals_std'] = 0\n",
    "\n",
    "    # Onset detection: where new notes begin\n",
    "    onsets = librosa.onset.onset_detect(y=audio_segment, sr=sr)\n",
    "    features['onset_count'] = len(onsets)\n",
    "\n",
    "    # Rhythm patterns using tempogram\n",
    "    tempogram = librosa_feature.tempogram(onset_envelope=onset_env, sr=sr)\n",
    "    features['tempogram_mean'] = np.mean(tempogram, axis=1)\n",
    "\n",
    "    # 3. HARMONIC AND MELODIC FEATURES\n",
    "\n",
    "    # Separate source audio segment into harmonic and percusive parts\n",
    "    harmonic, percussive = librosa.effects.hpss(audio_segment)\n",
    "\n",
    "    # Evaluates melodic similarities with harmonic audio part\n",
    "    chroma_stft = librosa_feature.chroma_stft(y=harmonic, sr=sr)\n",
    "    features['chroma_stft_mean'] = np.mean(chroma_stft, axis=1)\n",
    "    features['chroma_stft_std'] = np.std(chroma_stft, axis=1)\n",
    "\n",
    "    # Extract pitch related features with harmonic audio part (gets the tonal content)\n",
    "    chroma_cq = librosa_feature.chroma_cqt(y=harmonic, sr=sr)\n",
    "    features['chroma_cq_mean'] = np.mean(chroma_cq, axis=1)\n",
    "    features['chroma_cq_std'] = np.std(chroma_cq, axis=1)\n",
    "\n",
    "    # Tonnetz to measure harmonic relations (like how close 2 chords are on the circle of fifth)\n",
    "    tonnetz = librosa_feature.tonnetz(y=harmonic, sr=sr)\n",
    "    features['tonnetz_mean'] = np.mean(tonnetz, axis=1)\n",
    "    features['tonnetz_std'] = np.std(tonnetz, axis=1)\n",
    "\n",
    "    # 4. STRUCTURAL FEATURES\n",
    "\n",
    "    # Mel spectrogram (log scale)\n",
    "    # Represents the spectral energy across frequency bands, trying to get the overall feel of a song\n",
    "    mel_spec = librosa_feature.melspectrogram(y=audio_segment, sr=sr, n_mels=128)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "    features['mel_spec_mean'] = np.mean(log_mel_spec, axis=1)\n",
    "    features['mel_spec_std'] = np.std(log_mel_spec, axis=1)\n",
    "\n",
    "    # Zero-crossing rate (identifies signal crossing the 0 amplitude line)\n",
    "    # High ZCR: noisy, cymbals, Low ZCR: smooth, tonal sounds like vocals or flute for ex.\n",
    "    zcr = librosa_feature.zero_crossing_rate(audio_segment)[0]\n",
    "    features['zcr_mean'] = np.mean(zcr)\n",
    "    features['zcr_std'] = np.std(zcr)\n",
    "\n",
    "    # RMS energy (perceived loudness over time, measures energy dynamics)\n",
    "    rms = librosa_feature.rms(y=audio_segment)[0]\n",
    "    features['rms_mean'] = np.mean(rms)\n",
    "    features['rms_std'] = np.std(rms)\n",
    "\n",
    "    return features"
   ],
   "id": "debb61537a057143",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T21:48:48.425270Z",
     "start_time": "2025-04-16T21:48:48.418546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_features_from_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Process all song pairs and extract features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset : list\n",
    "        List of dictionaries containing pair_id, original_song_segments, copied_song_segments, and label\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of dictionaries with pair_id, original_features, copied_features, and label\n",
    "    \"\"\"\n",
    "    processed_dataset = []\n",
    "    os.makedirs(\"processed_features\", exist_ok=True)\n",
    "\n",
    "    for pair_data in tqdm(dataset, desc=\"Extracting features from song pairs\"):\n",
    "        pair_id = pair_data['pair_id']\n",
    "        original_segments = pair_data['original_song_segments']\n",
    "        copied_segments = pair_data['copied_song_segments']\n",
    "        label = pair_data['label']\n",
    "\n",
    "        # Extract features for each segment of the first song\n",
    "        original_features = []\n",
    "        for i, segment in enumerate(tqdm(original_segments, desc=f\"Processing original song {pair_id}\", leave=False)):\n",
    "            try:\n",
    "                features = extract_features(segment)\n",
    "                original_features.append(features)\n",
    "\n",
    "                # Save intermediate results every 5 segments\n",
    "                if (i + 1) % 5 == 0:\n",
    "                    with open(f\"processed_features/pair_{pair_id}_original_temp.pkl\", \"wb\") as f:\n",
    "                        pickle.dump(original_features, f)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing segment {i} in original song {pair_id}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Extract features for each segment of the second song\n",
    "        copied_features = []\n",
    "        for i, segment in enumerate(tqdm(copied_segments, desc=f\"Processing copied song {pair_id}\", leave=False)):\n",
    "            try:\n",
    "                features = extract_features(segment)\n",
    "                copied_features.append(features)\n",
    "\n",
    "                # Save intermediate results every 5 segments\n",
    "                if (i + 1) % 5 == 0:\n",
    "                    with open(f\"processed_features/pair_{pair_id}_copied_temp.pkl\", \"wb\") as f:\n",
    "                        pickle.dump(copied_features, f)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing segment {i} in copied song {pair_id}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Only add to processed dataset if both songs have features extracted\n",
    "        if original_features and copied_features:\n",
    "            processed_pair = {\n",
    "                'pair_id': pair_id,\n",
    "                'original_features': original_features,\n",
    "                'copied_features': copied_features,\n",
    "                'label': label\n",
    "            }\n",
    "            processed_dataset.append(processed_pair)\n",
    "\n",
    "            # Save each processed pair individually\n",
    "            with open(f\"processed_features/pair_{pair_id}_features.pkl\", \"wb\") as f:\n",
    "                pickle.dump(processed_pair, f)\n",
    "\n",
    "    return processed_dataset"
   ],
   "id": "acd21b97be7320c4",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T21:51:43.205013Z",
     "start_time": "2025-04-16T21:51:39.904597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load Dataset\n",
    "preprocessed_dataset = load_dataset_from_pickles(preprocessed_output_dir)\n",
    "print(f\"Loaded {len(preprocessed_dataset)} song pairs\")"
   ],
   "id": "a289662b4840b5fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 176 song pairs\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Start Extracting Features\n",
    "os.makedirs(\"features\", exist_ok=True)\n",
    "feat_processed_dataset = extract_features_from_dataset(preprocessed_dataset)\n",
    "with open(\"features/all_processed_features.pkl\", \"wb\") as f:\n",
    "        pickle.dump(feat_processed_dataset, f)"
   ],
   "id": "26e17e2c9471457e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h4><b>2.2 Prepare Feature Vectors for 1D CNN</b></h4>",
   "id": "d7b567ba36816505"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T22:57:49.273218Z",
     "start_time": "2025-04-16T22:57:49.256090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data_for_cnn(processed_dataset):\n",
    "    \"\"\"\n",
    "    Convert processed features to vectors suitable for 1D CNN input.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    processed_dataset : list\n",
    "        List of dictionaries with pair_id, original_features, copied_features, and label\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (X_pairs, y) where X_pairs contains paired feature vectors and y contains labels\n",
    "    \"\"\"\n",
    "    X_pairs = []\n",
    "    y = []\n",
    "\n",
    "    for pair_data in processed_dataset:\n",
    "        original_features = pair_data['original_features']\n",
    "        copied_features = pair_data['copied_features']\n",
    "        label = pair_data['label']\n",
    "\n",
    "        # For each pair of segments (one from original, one from copied) create a feature vector pair\n",
    "        for orig_feat in original_features:\n",
    "            for copy_feat in copied_features:\n",
    "                # Convert dictionary features to vector\n",
    "                orig_vector = []\n",
    "                copy_vector = []\n",
    "\n",
    "                # Add all numeric features to vectors\n",
    "                for key, value in orig_feat.items():\n",
    "                    if isinstance(value, np.ndarray):\n",
    "                        orig_vector.extend(value.flatten())\n",
    "                    elif isinstance(value, (int, float)):\n",
    "                        orig_vector.append(value)\n",
    "\n",
    "                for key, value in copy_feat.items():\n",
    "                    if isinstance(value, np.ndarray):\n",
    "                        copy_vector.extend(value.flatten())\n",
    "                    elif isinstance(value, (int, float)):\n",
    "                        copy_vector.append(value)\n",
    "\n",
    "                # Ensure vectors are of equal length by padding if need be\n",
    "                max_len = max(len(orig_vector), len(copy_vector))\n",
    "                orig_vector = np.pad(orig_vector, (0, max_len - len(orig_vector)), 'constant')\n",
    "                copy_vector = np.pad(copy_vector, (0, max_len - len(copy_vector)), 'constant')\n",
    "\n",
    "                # Create pair representation\n",
    "                X_pairs.append([np.array(orig_vector), np.array(copy_vector)])\n",
    "                y.append(label)\n",
    "\n",
    "    return np.array(X_pairs), np.array(y)"
   ],
   "id": "e339292d9a06f909",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T22:48:42.130002Z",
     "start_time": "2025-04-16T22:48:29.170021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data for 1D CNN\n",
    "os.makedirs(\"prepared_data_cnn\", exist_ok=True)\n",
    "X_pairs, y = prepare_data_for_cnn(feat_processed_dataset)\n",
    "with open(\"prepared_data_cnn/cnn_ready_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump((X_pairs, y), f)"
   ],
   "id": "acd9744fab0d1a52",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<br><h3><b>3. Training the Model (1D CNN with Cosine Similarity)</b></h3>",
   "id": "5f251e9888e4031"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:25:39.233257Z",
     "start_time": "2025-04-17T20:25:39.226405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "9ceb411f9d5489c5",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:25:39.997827Z",
     "start_time": "2025-04-17T20:25:39.956134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ],
   "id": "65df585297172404",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4><b>3.1 Class to help build the dataset for training loop.</b></h4><br>\n",
    "    - I'm using pairs of songs, so X1 holds the first songs from the pair, X2 holds the second songs from the pair.<br>\n",
    "    - y is simply the label (copyright case (1) or not (0))<br>\n",
    "    - This class will allow me to use the dataset with the Siamese model further below with PyTorch DataLoader.<br>"
   ],
   "id": "395b20d21e7656a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:25:41.905330Z",
     "start_time": "2025-04-17T20:25:41.901798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SongPairDataset(Dataset):\n",
    "    def __init__(self, X1, X2, y):\n",
    "        self.X1 = torch.tensor(X1, dtype=torch.float32)\n",
    "        self.X2 = torch.tensor(X2, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    # Gets the total number of song pairs.\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]"
   ],
   "id": "9c3c6cbdeff20bd4",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4><b>3.2 Siamese CNN Model</b></h4><br>\n",
    "I decided to use a Siamese CNN model since for my objective I need to compare how similar two inputs are.<br>\n",
    "This way, the Siamese model will learn an embedding space, where more similar songs will be closer to each other on this embedding space."
   ],
   "id": "85a772909d2064ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:25:43.447675Z",
     "start_time": "2025-04-17T20:25:43.443064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Siamese CNN Model\n",
    "class SiameseCNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SiameseCNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(32),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dim: [batch, 1, length]\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_once(x1)\n",
    "        out2 = self.forward_once(x2)\n",
    "        return out1, out2"
   ],
   "id": "fe17de11f7ce91d1",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<b>Cosine Similarity Loss Function</b><br>\n",
    "This will allow my model to learn and measure the similarity between 2 inputs.<br>\n",
    "Since my feature vectors are continuous values, I use MSE here."
   ],
   "id": "98b4646c4ddf7e22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:25:45.316339Z",
     "start_time": "2025-04-17T20:25:45.313783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_loss(out1, out2, lbl):\n",
    "    \"\"\"Computes the cosine loss for a pair of songs.\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "        out1: torch.Tensor\n",
    "            A batch of embeddings for the first songs in a song pair.\n",
    "        out2: torch.Tensor\n",
    "            A batch of embeddings for the second songs in a song pair.\n",
    "        lbl: torch.Tensor\n",
    "            A batch of labels (0 or 1) for similarity in a batch of song pairs.\n",
    "\n",
    "    RETURNS\n",
    "    -------\n",
    "        loss: torch.Tensor\n",
    "            The MSE between cosine similarity and supervised labels.\n",
    "    \"\"\"\n",
    "    cosine_sim = nn.functional.cosine_similarity(out1, out2)\n",
    "    return nn.functional.mse_loss(cosine_sim, lbl)  # 1 = similar, 0 = different"
   ],
   "id": "b4cd7d4b25f16949",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h4><b>3.3 Training the Model</b></h4>",
   "id": "7a2b7a03b871916c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<b>Train/Test Data Split</b>",
   "id": "f3e47959dc1004a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:25:48.846746Z",
     "start_time": "2025-04-17T20:25:47.550079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"prepared_data_cnn/cnn_ready_data.pkl\", \"rb\") as f:\n",
    "    X_pairs, y = pickle.load(f)\n",
    "\n",
    "X1 = np.array([pair[0] for pair in X_pairs])\n",
    "X2 = np.array([pair[1] for pair in X_pairs])\n",
    "y = np.array(y)\n",
    "\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(X1, X2, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = SongPairDataset(X1_train, X2_train, y_train)\n",
    "test_dataset = SongPairDataset(X1_test, X2_test, y_test)\n",
    "\n",
    "# Not that big a dataset, so batch size of 32 seemed good.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ],
   "id": "7d3d5afed8fda938",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<b>Training Loop</b>",
   "id": "fa41e6aefb41b227"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T21:08:53.184108Z",
     "start_time": "2025-04-17T20:28:29.301168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseCNN(input_dim=X1.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x1, x2, label in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "        out1, out2 = model(x1, x2)\n",
    "        loss = cosine_loss(out1, out2, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")"
   ],
   "id": "da55013e1f062739",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 2549/2549 [04:01<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.2059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 2549/2549 [04:02<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 0.1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 2549/2549 [03:59<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 0.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 2549/2549 [04:27<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss = 0.0354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 2549/2549 [03:57<00:00, 10.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 0.0237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 2549/2549 [04:00<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss = 0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 2549/2549 [03:59<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss = 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 2549/2549 [03:59<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss = 0.0109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 2549/2549 [03:58<00:00, 10.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss = 0.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 2549/2549 [03:56<00:00, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<b>Testing</b>",
   "id": "dc4dfc2f7f6001b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T21:10:40.007063Z",
     "start_time": "2025-04-17T21:09:46.417837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "threshold = 0.7  # Played around with this and the value seems best like this\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x1, x2, label in test_loader:\n",
    "        x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "        out1, out2 = model(x1, x2)\n",
    "        sim = nn.functional.cosine_similarity(out1, out2)\n",
    "        pred = (sim > threshold).float()\n",
    "        correct += (pred == label).sum().item()\n",
    "        total += label.size(0)\n",
    "\n",
    "print(f\"Test Accuracy: {correct / total:.2%}\")"
   ],
   "id": "10a3a32b72f2b11e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.40%\n"
     ]
    }
   ],
   "execution_count": 67
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
